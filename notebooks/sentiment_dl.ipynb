{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kSX_i9jbDJ3L"
   },
   "source": [
    "### **Sentiment Analysis â€“ Deep Learning (F:ull dataset)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yB5PmSRUAYOx"
   },
   "source": [
    "This notebook trains and compares **LSTM/BiLSTM** and **DistilBERT** models on the large sentiment dataset (1.6M tweets). The data is loaded directly from the remote URL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CJVCvAKcD4j4"
   },
   "source": [
    "**Data loader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bZxuSI3u6zgK",
    "outputId": "3287fab2-dca6-4a98-fef8-25ae3c80dcfb"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import requests, zipfile, io\n",
    "\n",
    "url = \"https://nyc3.digitaloceanspaces.com/ml-files-distro/v1/sentiment-analysis-is-bad/data/training.1600000.processed.noemoticon.csv.zip\"\n",
    "r = requests.get(url)\n",
    "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "df = pd.read_csv(z.open(\"training.1600000.processed.noemoticon.csv\"), encoding=\"latin-1\")\n",
    "df = df.sample(n=200000, random_state=42)\n",
    "\n",
    "# Datast columns\n",
    "df.columns = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "df = df[[\"target\", \"text\"]]\n",
    "\n",
    "# Convert target from 0=negative, 4=positive\n",
    "df[\"target\"] = df[\"target\"].replace({0:0, 4:1})\n",
    "\n",
    "print(df.head())\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QnCqgK_r8N5r"
   },
   "source": [
    "**Text cleaning (preprocessing)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i6jfPn-L8Ml9",
    "outputId": "27fcb06a-6da9-4294-c676-c15b318b746d"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)\n",
    "    text = re.sub(r\"[^a-z\\s]\", \"\", text)\n",
    "    tokens = [w for w in text.split() if w not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df[\"clean_text\"] = df[\"text\"].apply(clean_text)\n",
    "print(df[\"clean_text\"].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sq56QsvgEZIr"
   },
   "source": [
    "**LSTM tokenization and model build**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "id": "g85KZjAY8p-e",
    "outputId": "93539212-e0b8-4e72-99aa-452de2623388"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Data processing\n",
    "max_words = 20000\n",
    "max_len = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(df[\"clean_text\"])\n",
    "sequences = tokenizer.texts_to_sequences(df[\"clean_text\"])\n",
    "X = pad_sequences(sequences, maxlen=max_len)\n",
    "y = df[\"target\"].values\n",
    "\n",
    "# LSTM model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=max_words, output_dim=128),\n",
    "    LSTM(256, dropout=0.1, recurrent_dropout=0.1),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.1),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "\n",
    "model.build(input_shape=(None, max_len))\n",
    "\n",
    "optimizer = Adam(learning_rate=1e-3)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wdGnO4R6NiKc"
   },
   "source": [
    "**Model training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KusVauPSEyLe",
    "outputId": "a93fdc1b-4a1a-463b-d608-6416087217e8"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "history = model.fit(X_train, y_train, validation_split=0.2, epochs=5, batch_size=512)\n",
    "history_lstm = history\n",
    "\n",
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-xiT7mRZjbFB"
   },
   "source": [
    "**Confusion matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 427
    },
    "id": "M_Xak2SGNzjP",
    "outputId": "9dcf6112-eea0-4c76-af9e-fa0fb1c5cc29"
   },
   "outputs": [],
   "source": [
    "# Prediction (Keras LSTM)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Convert probabilities to categories\n",
    "y_pred = (y_pred > 0.5).astype(int).flatten()\n",
    "\n",
    "# Create confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix (LSTM)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ASIbs-X9ffk"
   },
   "source": [
    "## **DistilBERT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 926,
     "referenced_widgets": [
      "6b04cd74f15e4591a77ef033796cd992",
      "403037dd1b6e4c04bdab80980f3b96f0",
      "5e4ca49dfb014a939537f43697cfed1d",
      "3757d3014ba8438e8da3c47d3f031143",
      "4e75759b0f1343eabfd09f533c5c82f0",
      "8be18ede830e4eada73f576f5ddff2ef",
      "133ea93b0e4e47e9b6237e3e7f5d1047",
      "95e803ba27a7462cbcdf658627b5e2c1",
      "8456b452efe2471e81cc9b9154c1efce",
      "b9827d24d6b048119e11493e471e7887",
      "6ea8fabcf40341ed8a035186cea826be",
      "f0dede3dbb0a4d389a5ad20e95663923",
      "9077c9734523449aa311a01966745c1d",
      "0e3eb0bdfc954499ad753a7b360d8980",
      "1c3a5544ecae477ead6231d6eabee548",
      "cd5676e4133d492cbd75d052e373d13c",
      "5092cac1f95b4358b572ae00a3355894",
      "41523cca208f4253822db00fdb7bbf21",
      "6c0cbdd14f164b60973716c292409891",
      "70ab83297735412b9ad883feee948cbd",
      "a2ad38d6246247e1b4d7a160648515d4",
      "3975ec5610d44c578a7fa361a809b93c",
      "18cfc82764454b84a3dc91a5f1e931fa",
      "690090f8689e4c27a4578f4923172723",
      "54dd81fa7adb44dd9ec0301772914a0b",
      "a660e327fb004518a282fbc3627af0f0",
      "e9935e6afae64c3d9d10f09f9bc57ded",
      "a626247cf6e7444e8e2de5ab6f07f247",
      "d0c6b711e927461794e24bcd737b4fbe",
      "0653e9007012476ba93f7aff4c320efd",
      "e16b6549046946a2b75505c4ae536a1c",
      "4d02851d2111441eb43b54b4928e7f07",
      "2bf533563a2e40dd9776b8a3d7e0a767",
      "6489c5ba879c4f90abe3ec385a638cec",
      "0087821f75a2469282db9272a71c6062",
      "e7d604d1b0974a88bf661ef69b0efd19",
      "7d48f8ecbaac445784c4aac035d15be7",
      "34712cdfa6544d4aaf6db62e2841897c",
      "4d8e43796d124b44919c23abaf8efb7b",
      "0ccabf9d19f54e9aa28906f2c8fa63da",
      "036e114dbb8c407bbe5f1717c1a28834",
      "ce8ae30557fd4755905c79d2b39fd913",
      "29aaa971d6e74b02a41f082a9e05429c",
      "acad7a224cd94006bf9bb90a75486faf",
      "cf148632e01b40a795565d6fa05435c1",
      "8d37166fe81b4cf68a2b07451a727871",
      "99286d45515249fba6f08d12bf283df1",
      "ec44edddd6364913a74e9fb97cbcf466",
      "988cf4fe19b84e078728c500a2b0f58d",
      "5a4fca90766c48c2b7e1f4b2af8b9411",
      "e31e1900383d4adf977ae9573065008a",
      "a20f1bd46f55401ab62f98145852df4d",
      "2b15cca0689a4b02b954b7527c359e84",
      "c0e54754e33c4b88a61af6bb94997b3c",
      "2340027c0ce746849dfa6fecea28be56"
     ]
    },
    "id": "3Q6_z7iI9lv8",
    "outputId": "de34b503-7ec1-4866-e61f-c54857709481"
   },
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Data processing\n",
    "texts = df[\"clean_text\"].tolist()\n",
    "labels = df[\"target\"].tolist()\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    texts, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)\n",
    "val_encodings   = tokenizer(val_texts, truncation=True, padding=True, max_length=128)\n",
    "\n",
    "# Preparing the Dataset in PyTorch format\n",
    "class NewsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = NewsDataset(train_encodings, train_labels)\n",
    "val_dataset   = NewsDataset(val_encodings, val_labels)\n",
    "\n",
    "# DistilBERT model\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=2\n",
    ")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ZYIq12u_khV"
   },
   "source": [
    "**Model training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "id": "Xn--wQ1p_lFB",
    "outputId": "e5945a5e-eda3-473f-f33e-cf0639df1ec5"
   },
   "outputs": [],
   "source": [
    "# install evaluate if not already\n",
    "!pip install evaluate -q\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import transformers\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import traceback\n",
    "\n",
    "print(\"transformers version:\", transformers.__version__)\n",
    "\n",
    "# training kwargs\n",
    "training_kwargs = dict(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=200,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# Flexible: Tries to create TrainingArguments and does fallback between eval_strategy <-> evaluation_strategy\n",
    "def make_training_args(kwargs):\n",
    "    from transformers import TrainingArguments\n",
    "    try:\n",
    "        return TrainingArguments(**kwargs)\n",
    "    except TypeError as e:\n",
    "        kw = kwargs.copy()\n",
    "        if 'eval_strategy' in kw and 'evaluation_strategy' not in kw:\n",
    "            kw['evaluation_strategy'] = kw.pop('eval_strategy')\n",
    "        elif 'evaluation_strategy' in kw and 'eval_strategy' not in kw:\n",
    "            kw['eval_strategy'] = kw.pop('evaluation_strategy')\n",
    "        else:\n",
    "            raise\n",
    "        return TrainingArguments(**kw)\n",
    "\n",
    "try:\n",
    "    training_args = make_training_args(training_kwargs)\n",
    "except Exception as e:\n",
    "    print(\"Failed to create TrainingArguments:\")\n",
    "    traceback.print_exc()\n",
    "    raise\n",
    "\n",
    "# Download metric from the evaluate library\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return accuracy_metric.compute(predictions=preds, references=labels)\n",
    "\n",
    "# Create Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# training\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate and print the result\n",
    "eval_results = trainer.evaluate()\n",
    "if 'eval_accuracy' in eval_results:\n",
    "    print(f\"Test Accuracy: {eval_results['eval_accuracy']:.4f}\")\n",
    "else:\n",
    "    print(\"eval_results keys:\", eval_results.keys())\n",
    "    print(\"eval_results:\", eval_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7JWLbx55VXm1"
   },
   "source": [
    "**Confusion matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    },
    "id": "_CxIsr2KVYX0",
    "outputId": "d24e9ec2-0c85-4c75-bbb2-53f9692988f5"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Predict using val_dataset\n",
    "predictions = trainer.predict(val_dataset)\n",
    "\n",
    "# Extract forecasts\n",
    "y_pred = np.argmax(predictions.predictions, axis=1)\n",
    "\n",
    "y_true = predictions.label_ids\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# drawing the shape\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix (DistilBERT)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-a9rfKS5EoOP"
   },
   "source": [
    "## **Performance Comparison**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "id": "2u4H10YFKIc8",
    "outputId": "38a563fa-5e45-4dd1-95af-187c5c87a8f0"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# LSTM Validation Accuracy\n",
    "lstm_acc = max(history_lstm.history['val_accuracy'])\n",
    "\n",
    "# DistilBERT Validation Accuracy\n",
    "eval_results = trainer.evaluate(val_dataset)\n",
    "bert_acc = eval_results['eval_accuracy']\n",
    "\n",
    "print(f\"LSTM Validation Accuracy: {lstm_acc:.4f}\")\n",
    "print(f\"DistilBERT Validation Accuracy: {bert_acc:.4f}\")\n",
    "\n",
    "# Comparison drawing\n",
    "plt.bar(['LSTM', 'DistilBERT'], [lstm_acc, bert_acc], color=['#66FFFF', '#99FF99'])\n",
    "plt.title(\"Validation Accuracy Comparison\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
