{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis â€“ Deep Learning Notebook\n",
    "This notebook trains and evaluates LSTM/BiLSTM and DistilBERT models on the large sentiment analysis dataset (1.6M tweets).\n",
    "All data is loaded directly from the external source without saving locally."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Install necessary libraries\n",
    "!pip install transformers datasets nltk tensorflow scikit-learn matplotlib pandas" 
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from transformers import DistilBertTokenizerFast, TFDistilBertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load Dataset directly from external source\n",
    "url = 'https://nyc3.digitaloceanspaces.com/ml-files-distro/v1/sentiment-analysis-is-bad/data/training.1600000.processed.noemoticon.csv.zip'\n",
    "df = pd.read_csv(url, encoding='latin-1', header=None)\n",
    "df.columns = ['sentiment','id','date','query','user','text']\n",
    "df = df[['text','sentiment']]\n",
    "# Map sentiment: 0 -> negative, 2 -> neutral, 4 -> positive\n",
    "df['sentiment'] = df['sentiment'].map({0:0, 2:1, 4:2})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Text Preprocessing\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\s]', '', text)\n",
    "    text = ' '.join([w for w in text.split() if w not in stop_words])\n",
    "    return text\n",
    "\n",
    "df['clean_text'] = df['text'].apply(clean_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM / BiLSTM Model" 
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Tokenization and padding\n",
    "tokenizer = Tokenizer(num_words=20000)\n",
    "tokenizer.fit_on_texts(df['clean_text'])\n",
    "sequences = tokenizer.texts_to_sequences(df['clean_text'])\n",
    "X = pad_sequences(sequences, maxlen=50)\n",
    "y = df['sentiment'].values\n",
    "\n",
    "# Split train/validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build LSTM model\n",
    "model_lstm = Sequential([\n",
    "    Embedding(input_dim=20000, output_dim=128, input_length=50),\n",
    "    LSTM(128),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "model_lstm.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train LSTM\n",
    "history_lstm = model_lstm.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=2, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DistilBERT Model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Prepare data for DistilBERT\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(df['clean_text'], df['sentiment'], test_size=0.2, random_state=42)\n",
    "tokenizer_bert = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "train_enc = tokenizer_bert(list(train_texts), truncation=True, padding=True, max_length=128)\n",
    "val_enc = tokenizer_bert(list(val_texts), truncation=True, padding=True, max_length=128)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_enc), train_labels)).batch(16)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((dict(val_enc), val_labels)).batch(16)\n",
    "\n",
    "# Build DistilBERT model\n",
    "model_bert = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=3)\n",
    "optimizer = tf.keras.optimizers.Adam(5e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model_bert.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "# Train DistilBERT\n",
    "history_bert = model_bert.fit(train_dataset, validation_data=val_dataset, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "lstm_acc = history_lstm.history['val_accuracy'][-1]\n",
    "bert_acc = history_bert.history['val_accuracy'][-1]\n",
    "plt.bar(['LSTM','DistilBERT'], [lstm_acc, bert_acc], color=['blue','green'])\n",
    "plt.title('Validation Accuracy Comparison')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "- LSTM/BiLSTM provides a good baseline with reasonable accuracy.\n",
    "- DistilBERT achieves higher accuracy and better generalization on large datasets.\n",
    "- Visual comparison helps identify the best model for deployment."
   ]
  }
 ],
 "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.12"}},
 "nbformat": 4,
 "nbformat_minor": 5
}
