{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7da41de",
   "metadata": {},
   "source": [
    "# Sentiment Analysis â€“ Deep Learning (Full dataset)\n",
    "\n",
    "This notebook trains and compares **LSTM/BiLSTM** and **DistilBERT** models on the large sentiment dataset (1.6M tweets). The data is loaded directly from the remote URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43906de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries (run this first in Colab or local environment)\n",
    "!pip install --quiet transformers datasets nltk tensorflow scikit-learn matplotlib pandas\n",
    "\n",
    "# Note: Installing may take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd0db28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from transformers import DistilBertTokenizerFast, TFDistilBertForSequenceClassification\n",
    "from datasets import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebbcc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, zipfile, io\n",
    "url = 'https://nyc3.digitaloceanspaces.com/ml-files-distro/v1/sentiment-analysis-is-bad/data/training.1600000.processed.noemoticon.csv.zip'\n",
    "print('Downloading dataset (this may take a few minutes)...')\n",
    "r = requests.get(url, stream=True)\n",
    "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "csv_filename = z.namelist()[0]\n",
    "df = pd.read_csv(z.open(csv_filename), encoding='latin-1', header=None)\n",
    "df.columns = ['sentiment','id','date','query','user','text']\n",
    "df = df[['text','sentiment']]\n",
    "# Map sentiment: 0 -> negative, 2 -> neutral, 4 -> positive\n",
    "df['sentiment'] = df['sentiment'].map({0:0, 2:1, 4:2})\n",
    "print('Loaded dataframe shape:', df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87d1d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'http\\S+','', text)             # remove urls\n",
    "    text = re.sub(r'[^a-z\\s]','', text)            # keep letters and spaces\n",
    "    text = ' '.join([w for w in text.split() if w not in stop_words])\n",
    "    return text\n",
    "\n",
    "print('Cleaning texts (this may take several minutes for 1.6M rows)...')\n",
    "df['clean_text'] = df['text'].apply(clean_text)\n",
    "df[['clean_text','sentiment']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efa0ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM tokenization and model build\n",
    "vocab_size = 20000\n",
    "max_len = 50\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(df['clean_text'])\n",
    "sequences = tokenizer.texts_to_sequences(df['clean_text'])\n",
    "X = pad_sequences(sequences, maxlen=max_len)\n",
    "y = df['sentiment'].values\n",
    "\n",
    "print('X shape, y shape:', X.shape, y.shape)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model_lstm = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=128, input_length=max_len),\n",
    "    LSTM(128),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "model_lstm.build(input_shape=(None, max_len))\n",
    "model_lstm.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_lstm.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f9697c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LSTM (adjust epochs/batch_size based on available resources)\n",
    "lstm_epochs = 3\n",
    "lstm_batch = 256\n",
    "history_lstm = model_lstm.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=lstm_epochs, batch_size=lstm_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374dadfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset for DistilBERT using Hugging Face datasets (efficient format)\n",
    "ds = Dataset.from_pandas(df[['clean_text','sentiment']].rename(columns={'sentiment':'label'}))\n",
    "print('Dataset length =', len(ds))\n",
    "ds = ds.train_test_split(test_size=0.2)\n",
    "\n",
    "tokenizer_bert = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer_bert(batch['clean_text'], padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "print('Tokenizing dataset (this will take time)...')\n",
    "tokenized = ds.map(tokenize_fn, batched=True, remove_columns=['clean_text'])\n",
    "tokenized = tokenized.rename_column('label','labels')\n",
    "tokenized.set_format(type='tensorflow', columns=['input_ids','attention_mask','labels'])\n",
    "\n",
    "tf_train = tokenized['train'].to_tf_dataset(columns=['input_ids','attention_mask'], label_cols='labels', shuffle=True, batch_size=16)\n",
    "tf_val   = tokenized['test'].to_tf_dataset(columns=['input_ids','attention_mask'], label_cols='labels', shuffle=False, batch_size=16)\n",
    "print('tf_train, tf_val created')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af519495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build DistilBERT model for sequence classification\n",
    "model_bert = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=3)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model_bert.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "model_bert.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1393b846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train DistilBERT (this is heavy; consider using GPU/TPU in Colab)\n",
    "bert_epochs = 1\n",
    "history_bert = model_bert.fit(tf_train, validation_data=tf_val, epochs=bert_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dccdc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare validation accuracies\n",
    "lstm_val_acc = history_lstm.history.get('val_accuracy', [None])[-1]\n",
    "bert_val_acc = history_bert.history.get('val_accuracy', [None])[-1]\n",
    "print('LSTM validation accuracy:', lstm_val_acc)\n",
    "print('BERT validation accuracy:', bert_val_acc)\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.bar(['LSTM','DistilBERT'], [lstm_val_acc or 0, bert_val_acc or 0])\n",
    "plt.ylim(0,1)\n",
    "plt.title('Validation Accuracy Comparison')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318a365a",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "- LSTM/BiLSTM provides a good baseline and may train faster on CPU/GPU depending on resources.\n",
    "- DistilBERT typically achieves higher accuracy and better generalization but requires significantly more memory and compute.\n",
    "- For full 1.6M training, use a powerful GPU/TPU runtime (Colab Pro/TPU or dedicated cloud instances).\n",
    "\n",
    "### Notes\n",
    "- If you run into memory issues, reduce batch sizes or train on a subset when experimenting locally.\n",
    "- You can upload this notebook directly to your GitHub repository and open it in Colab using: `Open in Colab`.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
