# ‚ú® Sentiment Analysis ‚Äì Deep Learning Notebook Version

![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)
![Python](https://img.shields.io/badge/Python-3.10+-blue.svg?logo=python)
![NLP](https://img.shields.io/badge/NLP-TensorFlow-green.svg)
![Transformers](https://img.shields.io/badge/Transformers-HuggingFace-orange)
![IDE-VSCode](https://img.shields.io/badge/IDE-VS%20Code-007ACC?logo=visualstudiocode&logoColor=white)
![Platform-Colab](https://img.shields.io/badge/Notebook-Colab-blue)
![Status](https://img.shields.io/badge/Status-In%20Progress-yellow)

<!-- Libraries -->
![pandas](https://img.shields.io/badge/pandas-1.6.2-blue?logo=pandas&logoColor=white)
![numpy](https://img.shields.io/badge/numpy-2.0.2-blue?logo=numpy&logoColor=white)
![scikit-learn](https://img.shields.io/badge/scikit--learn-1.2.3-orange?logo=scikit-learn&logoColor=white)
![nltk](https://img.shields.io/badge/nltk-3.8.1-blue)
![matplotlib](https://img.shields.io/badge/matplotlib-3.8.0-red?logo=matplotlib&logoColor=white)
![tensorflow](https://img.shields.io/badge/TensorFlow-2.15.0-orange?logo=tensorflow&logoColor=white)
![datasets](https://img.shields.io/badge/datasets-4.0.0-lightgrey)
![evaluate](https://img.shields.io/badge/evaluate-0.4.6-lightgrey)

---

## Overview
This project is an **enhanced version** of the original Sentiment Analysis notebook.  
It classifies text data (tweets, reviews, or short sentences) into **positive, negative, or neutral** sentiments using **Deep Learning** (LSTM / BiLSTM) and **Transformer models** (DistilBERT).  
The goal is to provide a **scalable, modern NLP solution** demonstrating the differences between classical deep learning and pretrained transformer architectures.

---

## üéØ Project Goals
- Apply advanced NLP techniques for sentiment classification.
- Compare **LSTM / BiLSTM** with **DistilBERT** models.
- Train on a **large-scale dataset** (~1.6M tweets) for better accuracy.
- Showcase preprocessing, model training, evaluation, and visual comparison.
- Provide a professional, shareable AI portfolio piece.

---

## üì• Dataset
- **Sentiment Analysis Dataset (1.6M tweets)**: [Download ZIP](https://nyc3.digitaloceanspaces.com/ml-files-distro/v1/sentiment-analysis-is-bad/data/training.1600000.processed.noemoticon.csv.zip)

---

## üÜö Version Comparison

| Feature | Version 1 | Version 2 |
|---------|-----------|-----------|
| Dataset size | ~15,000 tweets | 1,600,000 tweets |
| Model | Logistic Regression | LSTM / BiLSTM + DistilBERT |
| Text preprocessing | Basic cleaning | Full cleaning + Tokenization for Transformers |
| Accuracy | ~78% | Higher, especially with BERT |
| Scalability | Limited | Can handle large datasets |
| Evaluation | Simple metrics | Comparison of LSTM vs DistilBERT with visualizations |

---

## üì¶ Project Structure


```bash
üìÅSentiment_Analysis_DeepLearning/
‚îÇ
‚îú‚îÄüìÅ notebooks/
‚îÇ ‚îî‚îÄ‚îÄ sentiment_dl.ipynb  # Main Colab/Jupyter notebook
‚îÇ
‚îú‚îÄüìÅ src/
‚îÇ ‚îú‚îÄ‚îÄ data_loader.py      # Download dataset from external link
‚îÇ ‚îú‚îÄ‚îÄ preprocess.py       # Text cleaning and preprocessing
‚îÇ ‚îú‚îÄ‚îÄ lstm_model.py       # Build LSTM model
‚îÇ ‚îú‚îÄ‚îÄ bert_model.py       # Build DistilBERT model
‚îÇ ‚îî‚îÄ‚îÄ train.py            # Training script for any model
‚îÇ
‚îú‚îÄ‚îÄ main.py               # Main Python code
‚îú‚îÄ‚îÄ requirements.txt      # Required Python packages
‚îú‚îÄ‚îÄ LICENSE               # LICENSE file
‚îî‚îÄ‚îÄ README.md             # This file
```


---


## ‚öôÔ∏è How to Run

### 1Ô∏è‚É£ LSTM / BiLSTM Experiment
```bash
python src/train.py lstm
```
- Loads the dataset, cleans text, builds LSTM, trains, and evaluates.
- Prints validation/test accuracy.

### 2Ô∏è‚É£ DistilBERT Experiment
```bash
python src/train.py bert
```

- Loads the dataset, tokenizes with HuggingFace tokenizer, builds DistilBERT, trains, and evaluates.

- Usually achieves higher accuracy than LSTM.

### 3Ô∏è‚É£ Notebook

Open notebooks/sentiment_dl.ipynb in Google Colab or Jupyter Notebook.
- Contains full experiments for:
  - LSTM / BiLSTM
  - DistilBERT
- Includes visual comparison of model performance.

## Requirements (requirements.txt)
- Python >= 3.8

 **Data processing**
- pandas
- numpy
- scikit-learn
- requests

 **NLP & Transformers**
- torch
- torchvision
- torchtext
- transformers
- evaluate

 **Text preprocessing**
- nltk
- spacy

 **Visualization**
- matplotlib
- seaborn

 **Progress bars**
- tqdm

 **Optional: for downloading large datasets**
- urllib3

- datasets

Install all requirements with:
```bash
pip install -r requirements.txt
```

or manually:
```bash
pip install pandas numpy scikit-learn nltk matplotlib tensorflow transformers datasets
```


## üìà Model Performance

- LSTM / BiLSTM: Good baseline results, suitable for smaller resources.

- DistilBERT: Higher accuracy, better generalization, especially on large datasets.

 **Example**

| Model         | Validation Accuracy |
|---------------|---------------------|
| LSTM          | 0.85                |
| DistilBERT    | 0.92                |

 **Visual comparison available in the notebook (sentiment_dl.ipynb).**

## üß† Future Plans

- Extend to multilingual datasets (Arabic / English).

- Deploy models with Streamlit or Gradio for real-time predictions.

- Experiment with other transformers (BERT, RoBERTa, XLNet).

- Optimize LSTM for faster inference on resource-limited devices.

## Conclusion
Version 2 represents a significant improvement over the first version because it uses:
- Larger dataset ‚Üí better learning and model performance.
- Modern Deep Learning models ‚Üí LSTM + DistilBERT.
- Performance comparison ‚Üí identifies the best model based on accuracy and resource usage.

---

## üìä Visual Outputs

### 1Ô∏è‚É£ Data Loading
![Data Loader](images/Data_loader.jpg)

### 2Ô∏è‚É£ LSTM Model Build & Training
![LSTM Tokenization & Model Build](images/LSTM_tokenization_and_model_build.jpg)

### LSTM Model Training
![LSTM Model Training](images/LSTM_Model_training.jpg)

### LSTM Confusion Matrix
![LSTM Confusion Matrix](images/LSTM_Confusion_matrix.jpg)

### 3Ô∏è‚É£ DistilBERT Model Build & Training

### DistilBERT Model Training
![DistilBERT Model Training](images/DistilBERT_Model_training.jpg)

### DistilBERT Confusion Matrix
![DistilBERT Confusion Matrix](images/DistilBERT_Confusion_matrix.jpg)

### 4Ô∏è‚É£ Performance Comparison
![Performance Comparison](images/Performance_Comparison.jpg)

---

## üìå Author
### Omar Khamis
AI & Robotics Enthusiast | Python Developer
- üíº [LinkedIn](https://www.linkedin.com/in/omar-khamis-dev)
 | üíª [GitHub](https://github.com/omar-khamis-dev)
- üìß Email: omar.khamis.dev@gmail.com

---

## üìú License
This project is licensed under the **Apache 2.0 License** ‚Äì see the [LICENSE](./LICENSE) file for details.
